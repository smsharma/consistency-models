{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c551e8f4-6672-428b-abd0-6154a325732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import optax\n",
    "from tqdm import tqdm, trange\n",
    "import flax\n",
    "\n",
    "import math\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure TF does not see GPU and grab all GPU memory\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "from models.mlp_mixer import MLPMixer\n",
    "from models.consistency_utils import timestep_embedding, timestep_discretization\n",
    "from models.train_utils import TrainState, apply_ema_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9845929f-b9ac-4ad7-b32e-3c91e90ce7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "num_batch = 64\n",
    "d_t_emb = 16\n",
    "sigma_data = 0.5\n",
    "s0 = 2\n",
    "s1 = 150\n",
    "sigma = 7.\n",
    "mu0 = 0.9\n",
    "eps = 0.002\n",
    "T = 80.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbf85f48-c44d-4887-9930-b6e9f9712e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_dataset(split, batch_size):\n",
    "    \"\"\"Returns an iterator over the MNIST dataset.\"\"\"\n",
    "    # Load the dataset and prepare it for batching\n",
    "    ds = tfds.load('mnist', split=split, shuffle_files=True if split == \"train\" else False)\n",
    "    ds = ds\n",
    "    \n",
    "    # Define a function to preprocess each batch\n",
    "    def preprocess(batch):\n",
    "        mean, std = 0.1307, 0.3081\n",
    "        images = batch['image']\n",
    "        # Flatten the images and normalize their pixel values to [0, 1]\n",
    "        images = tf.cast(images, tf.float32) / 255\n",
    "        images = 2. * images - 1.\n",
    "        labels = batch['label']\n",
    "        return images, labels\n",
    "    \n",
    "    # Map the preprocessing function to each batch\n",
    "    ds = ds.map(preprocess).batch(batch_size).cache().repeat()\n",
    "    \n",
    "    # Return an iterator over the preprocessed batches\n",
    "    return iter(ds)\n",
    "\n",
    "batches = get_mnist_dataset(\"train\", num_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54c188fc-d1ee-403c-997f-239d9363086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_theta(params, score, x, t, y):\n",
    "    \"\"\" The consistency model.\n",
    "    \"\"\"\n",
    "\n",
    "    c_skip = sigma_data ** 2 / ((t - eps) ** 2 + sigma_data ** 2)\n",
    "    c_out = sigma_data * (t - eps) / np.sqrt(sigma_data ** 2 + t ** 2)\n",
    "    \n",
    "    if len(t.shape) > 1:\n",
    "        t = t[..., 0]\n",
    "    \n",
    "    t = timestep_embedding(t, d_t_emb)\n",
    "    x_out = score.apply(params, x, t, y)\n",
    "\n",
    "    return x * c_skip[:, :, None, None] + x_out * c_out[:, :, None, None]\n",
    "\n",
    "@partial(jax.jit, static_argnums=(4,5))\n",
    "def train_step(state, batch, t, key, model, loss_fn):\n",
    "    \"\"\" Single train step.\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = batch\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params, x_batch, t, key, model, y_batch)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "    \n",
    "@partial(jax.jit, static_argnums=(4,))\n",
    "def loss_fn(params, x, t, key, score, y):\n",
    "    \"\"\"Continous consistency loss function.\"\"\"\n",
    "    \n",
    "    def f_theta_unbatched(params, score, x, t, y):\n",
    "        x = x[None, ...]\n",
    "        t = t[None, ...]\n",
    "        y = y[None, ...]\n",
    "        out = f_theta(params, score, x, t, y)[0, ...]\n",
    "        return out\n",
    "\n",
    "    z = jax.random.normal(key, shape=x.shape)\n",
    "    xt = x + z * t[:, :, None, None]\n",
    "\n",
    "    params_min = jax.lax.stop_gradient(params)\n",
    "\n",
    "    f_theta_vmapped = jax.vmap(f_theta_unbatched, in_axes=(None, None, 0, 0, 0))(params, score, x, t, y)\n",
    "    d_f_theta_dx, d_f_theta_dt = jax.vmap(jax.jacfwd(f_theta_unbatched, argnums=(2, 3)), in_axes=(None, None, 0, 0, 0))(params_min, score, x, t, y)\n",
    "\n",
    "    loss2 = d_f_theta_dt[..., 0] - np.einsum(\"bhwcijk,bijk->bhwc\", d_f_theta_dx, (xt - x) / t[:, None, None])\n",
    "\n",
    "    return 2 * np.mean(np.einsum(\"bijk,bijk->b\", f_theta_vmapped, loss2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0238709c-c5a6-45bd-89a8-eabafa9fb987",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = MLPMixer(patch_size=4, num_blocks=4, hidden_dim=256, tokens_mlp_dim=256, channels_mlp_dim=256, num_classes=10)\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "x = jax.random.normal(key, (4, 28, 28, 1))\n",
    "t = np.ones((4, d_t_emb))\n",
    "context = np.ones((4,)).astype(np.int32)\n",
    "\n",
    "params = score.init(key, x, t, context)\n",
    "\n",
    "tx = optax.adamw(learning_rate=3e-4, weight_decay=1e-4)\n",
    "state = TrainState.create(apply_fn=score.apply, params=params, tx=tx, params_ema=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24321985-1bf9-4365-b20e-427fcba78314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10000/10000 [38:35<00:00,  4.32it/s, val=-1325.5435]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10_000\n",
    "\n",
    "with trange(num_steps) as steps:\n",
    "    for step in steps:\n",
    "        \n",
    "        # Get a data batch\n",
    "        x_batch, y_batch = next(batches)\n",
    "        x_batch, y_batch = x_batch._numpy(), y_batch._numpy()\n",
    "                \n",
    "        # Draw timesteps from discretized schedule\n",
    "        key, _ = jax.random.split(key)\n",
    "        t_batch = jax.random.uniform(key, minval=0, maxval=T, shape=(x_batch.shape[0], 1))\n",
    "        \n",
    "        key, _ = jax.random.split(key)\n",
    "        state, loss = train_step(state, (x_batch, y_batch), t_batch, key, score, loss_fn)\n",
    "\n",
    "        steps.set_postfix(val=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72fd8dcc-98fc-4980-844f-8a22cd245d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(params, score, timesteps, key, label):\n",
    "    \"\"\" Draw a sample from consistency model.\n",
    "    \"\"\"\n",
    "    x0 = jax.random.normal(key, shape=(n_samples, 28, 28, 1)) * timesteps[0]\n",
    "    \n",
    "    x = f_theta(params, score, x0, np.repeat(timesteps[0], x0.shape[0])[:, None], y)\n",
    "    for t in timesteps[1:]:\n",
    "        key, _ = jax.random.split(key)\n",
    "        z = jax.random.normal(key, shape=x0.shape)\n",
    "        x = x + math.sqrt(t ** 2 - eps ** 2) * z\n",
    "        x = f_theta(params, score, x, np.repeat(t, x0.shape[0])[:, None], y)\n",
    "    return x\n",
    "\n",
    "n_samples = 16\n",
    "key = jax.random.PRNGKey(420)\n",
    "\n",
    "y = np.arange(16) % 10\n",
    "\n",
    "# Generate samples with 5 and 2 steps\n",
    "x_samples_5 = sample(state.params, score, list(reversed([1., 5., T / 4., T / 2., T])), key, y)\n",
    "x_samples_2 = sample(state.params, score, list(reversed([1., T])), key, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94432a6b-a7e5-4c51-bb2b-babed3fd084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "x_samples_5_plot = rearrange(x_samples_5, \"(n1 n2) h w 1 -> (n1 h) (n2 w)\", n1=4)\n",
    "x_samples_2_plot = rearrange(x_samples_2, \"(n1 n2) h w 1 -> (n1 h) (n2 w)\", n1=4)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.imshow(x_samples_5_plot, cmap=\"binary_r\")\n",
    "ax1.axis(\"off\")\n",
    "ax1.set_title(\"5-step samples\", fontsize=16)\n",
    "\n",
    "ax2.imshow(x_samples_2_plot, cmap=\"binary_r\")\n",
    "ax2.axis(\"off\")\n",
    "ax2.set_title(\"2-step samples\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"plots/mnist_samples.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0fdbb-a225-4b6f-b546-b905ed117666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
